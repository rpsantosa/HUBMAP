---
title: "R Notebook"
output: html_notebook
---


```{r}
#####################################################################################################################################################################################################
# Important!!  image_read/image_write ( magick) preserves the figure format, both matrix and png. (nc,nl)--(nc,nl)   
#             load.image/ save.image (imager) transpose when load and when saves. The matrix form is transposed form figure (nl,nc) -- (nc,nl)

# sp::point.in.polygon() returns if a point is inside a polygon
#0: point is strictly exterior to pol; 
#1: point is strictly interior to pol;
#2: point lies on the relative interior of an edge of pol;
#3: point is a vertex of pol.

#image_crop: "2x2+0+0" extracts a[1:2,1:2]. "2x2+1+1" extracts a[2:3,2:3]

#####################################################################################################################################################################################################

```



```{r message=FALSE, warning=FALSE}
# libraries we're going to need later
# library(keras)
# library(tfdatasets)
library(tidyverse)
library(dplyr)
# library(rsample)
# library(reticulate)
library(jsonlite)
library(magick)
# library(cowplot)
# library(raster)
library(imager)
library(utils)
library(sp)

TRAIN<- file.path("c:/kaggletemp/HuBMAP/train_images/")
MASK<- file.path("c:/kaggletemp/HuBMAP/mask_images/")
TEST<- file.path("c:/kaggletemp/HuBMAP/test/")
BASE<-file.path("c:/kaggletemp/HuBMAP/")
BASET<-file.path("c:/kaggletemp/HuBMAP/train/")

```
```{r message=FALSE, warning=FALSE}
train<- read_delim(file.path(BASE,"train.csv"), delim = ",")
test<-read_csv(file.path(BASE,"sample_submission.csv"))
dataset<- read_csv(file.path(BASE,"HuBMAP-20-dataset_information.csv"))



```


```{r}

baseid<-list.files(BASET, full.names = F,pattern = "*.tiff") %>% gsub(pattern = ".tiff",replacement =  "")
anato<- file.path(BASET, paste0(baseid,"-anatomical-structure.json"))
img<-file.path(BASET, paste0(baseid,".tiff"))
jsons<- file.path(BASET, paste0(baseid,".json"))
jsonst<-list.files(TEST, full.names=T, pattern = "*.json")  

dj <- tibble(
  jsons= lapply(jsons ,fromJSON),
  anato = lapply(anato,fromJSON)
)
dt <- tibble(
  anato = lapply(jsonst,fromJSON)
)

```



```{r}
# a<- image_read(img[4]) 
# image_info(a)
# # fplot<-function(img){
# # img %>% image_read %>% image_scale("512") %>% image_rotate(90) %>% image_trim %>% plot()
# # }
# a  %>%  image_scale("512") %>% image_rotate(90) %>% image_trim %>% plot()
```

```{r}
# limg <- map (1:8, function(i) {
#   img[i]%>% image_read  %>%  image_scale("128x") %>% image_rotate(0) %>% image_trim %>% plot()
#   
# })
# plot_grid(plotlist = limg, nrow =2)
```


```{r}
# mask2rle<- function(x){
#    pixel <- x %>% as.vector
#    xx<- rle(pixel)
#    ids<- which (xx$values != 0 )
#    lls<- xx$lengths[ids]
#    cs<- cumsum(xx$lengths)
#    css<-cs[ids-1]+1
#    return( paste(css,lls,sep = " ", collapse = " ") )
# }

rle2mask <- function(mask_rle,shape){
  sl<- mask_rle   %>% strsplit(" ") %>% .[[1]] %>% as.numeric
  starts<-sl[seq(1,(length(sl) -1),2)] 
  lengths<-sl[seq(2,length(sl),2)]
  ends<-starts + lengths -1
  img<- matrix(0,ncol= shape[1] ,nrow=shape[2]  )
  #img<-imfill(0,x=shape[2],y=shape[1],z=1)
  ids<-lapply(1:length(starts),function(i){seq(starts[i],ends[i],1)}) %>% unlist
  img[ids]<-1
 # dim(img)<-c(shape[2],shape[1])
  #img<-  img %>%t #%>%imager::save.image("test.png")
  dim(img)<-c(dim(img),1)
    # img %>% image_read() %>% image_write("magictest.png", format = "png")
  img<- img %>% image_read()
  return((img))
}

# ffnames<-list.files(TRAIN, full.names = F,pattern = "*.tiff") %>% gsub(pattern = ".tiff",replacement =  "")

# mask_rle<- train %>% subset(id == ffnames[8]) %>% dplyr::select(encoding) %>% pull
# shape_rle<-dataset %>% subset(image_file == paste0(ffnames[8], ".tiff")) %>% dplyr::select(width_pixels, height_pixels) %>% unlist %>% as.vector
# 
# mask<- rle2mask(mask_rle, shape_rle)
# md<- dim(mask)
# dim(mask)<-c(md[1],md[2],1)
#y<- mask2rle(x)


```


```{r}



# a<- image_read( grep(ffnames[8],img, value =T)) %>% magick2cimg() 
# b<- mask %>% as.cimg %>%   as.pixset()
# 
# plot(a)
# boundary(b) %>% where %$% { points(x,y,cex=.1,col="red") }
# 



```
```{r}
# makeMask<-function(i){
#   
# mask_rle<- train %>% subset(id == ffnames[i]) %>% dplyr::select(encoding) %>% pull
# shape_rle<-dataset %>% subset(image_file == paste0(ffnames[i], ".tiff")) %>% dplyr::select(width_pixels, height_pixels) %>% unlist %>% as.vector
# mask<- rle2mask(mask_rle, shape_rle)
# md<- dim(mask)
# dim(mask)<-c(md[2],md[1],md[3])
# out<-image_read(mask)
# return(out)
# 
# }

```


The data is too heavy. Lets split it in several smaller images, with overlap

```{r}
fp<- function(k){
  ff<-function(xx){
  out<- xx  %>% .[1,,] %>% as.data.frame
  }
  pnames<-dj$anato[[k]]$properties[,1][,1]
  ptype<-  dj$anato[[k]]$geometry$type
  geop<- dj$anato[[k]]$geometry$coordinates
  ip<- which(pnames== "Cortex")
  cort<-list()
  for(ipx in ip){
    if(ptype[ipx]=="Polygon"){
      cort<-c(list(ff(geop[[ipx]])),cort)
    }else{
      cort<-c(map(geop[[ipx]],ff),cort)
    }
  }
  return(cort)
}
  
isCortex<-function(i,j,k){
  vv<-list(c(i+1,j+1),c(i+SPLIT,j+1),c(i,j+ SPLIT),c(i+SPLIT,j+SPLIT))
  pp<-fp(k)
  res<-0
  for(v in 1:length(vv)){
    for(p in 1:length(pp)){
      res<-res+ point.in.polygon(vv[[v]][1],vv[[v]][2],pp[[p]][,1],pp[[p]][,2])
    }
  }
return(res)
}

slices<-function(k,SPLIT){
  mask_rle<- train %>% subset(id == baseid[k]) %>% dplyr::select(encoding) %>% pull
  shape<-dataset %>% subset(image_file == paste0(baseid[k], ".tiff")) %>% dplyr::select(width_pixels, height_pixels) %>% unlist %>% as.vector
  ss<-ceiling(shape/SPLIT)
  return(ss[1] * ss[2])
}
sls<- map(1:length(baseid),slices,SPLIT=SPLIT)
total<- sum(sls %>% unlist)
```
##Better way to split things


```{r}

SPLIT =512
patht<-file.path(BASE,"train_images")
if(!dir.exists(patht)) {
dir.create(patht)
}
pathm<-file.path(BASE,"mask_images")
if(!dir.exists(pathm)) {
dir.create(pathm)
}

total<- sum(sls %>% unlist)
  
er =0
pb <- txtProgressBar(min = 0, max = total, style = 3)
makePng<-function(i,j,im=im, mask=mask,k=k){
      if(isCortex(i,j,k)>0){
  
      dest<- paste0(SPLIT,"x",SPLIT, "+",i, "+", j)
      image_write( image_crop(im,dest), format = "png",
            path = file.path(patht, paste0("image","_" ,baseid[k],"_",i,"_",j,"_" ,".png") ))
      
      image_write( image_crop(mask,dest), format = "png",
            path = file.path(pathm, paste0("mask","_" ,baseid[k],"_",i,"_",j,"_" ,".png") ))
      }
    # imager::save.image(
    #   imsub(im, x %inr% c(i+1,i+SPLIT), y %inr% c(j+1,j+SPLIT)),
    #     file = file.path(patht, paste0("image","_" ,ffnames[k],"_",i,"_",j,"_" ,".png") ))
  
     # imager::save.image(
     #  imsub(mask, x %inr% c(i+1,i+SPLIT), y %inr% c(j+1,j+SPLIT)),
     #  file = file.path(pathm, paste0("mask","_" ,ffnames[k],"_",i,"_",j, "_" ,".png") ))
  er<<-er +1
  if(er %% 100==0){
    gc()}
setTxtProgressBar(pb, er)
}
  
makeGrid<-function(k,SPLIT=SPLIT){
  mask_rle<- train %>% subset(id == baseid[k]) %>% dplyr::select(encoding) %>% pull
  shape<-dataset %>% subset(image_file == paste0(baseid[k], ".tiff")) %>% dplyr::select(height_pixels,width_pixels ) %>% unlist %>% as.vector
  nl<-c( seq(0,shape[1]-SPLIT, SPLIT), shape[1]-SPLIT)
  nc<-c( seq(0,shape[2]-SPLIT, SPLIT) , shape[2]-SPLIT)
  nn<-expand.grid(nl,nc)
  
  im<- image_read(file.path(BASET,paste0(baseid[k],".tiff"))) #%>% .[[1]] %>% as.numeric # %>% aperm(perm = c(2,1,3))
  gc()
  mask<-rle2mask(mask_rle, shape) 
  gc()
  walk2(nn[,1],nn[,2],makePng,im=im,mask=mask,k=k)
}

walk(1:nrow(train),makeGrid,SPLIT=SPLIT)

  

```




```{r}
generator <- function(data,shuffle = FALSE, batch_size = 128, step = 6) {
  if (is.null(max_index))
    max_index <- nrow(data) - delay - 1
  i <- min_index + lookback
  function() {
    if (shuffle) {
      rows <- sample(c((min_index+lookback):max_index), size = batch_size)
    } else {
      if (i + batch_size >= max_index)
        i <<- min_index + lookback
      rows <- c(i:min(i+batch_size-1, max_index))
      i <<- i + length(rows)
    }

    samples <- array(0, dim = c(length(rows),
                                lookback / step,
                                dim(data)[[-1]]))
    targets <- array(0, dim = c(length(rows)))

    for (j in 1:length(rows)) {
      indices <- seq(rows[[j]] - lookback, rows[[j]]-1,
                     length.out = dim(samples)[[2]])
      samples[j,,] <- data[indices,]
      targets[[j]] <- data[rows[[j]] + delay,2]
    }
    list(samples, targets)
  }
}
     k = 1
    SIZE = 2^9
    shape<-dataset %>% subset(image_file == paste0(ffnames[6], ".tiff")) %>% dplyr::select(width_pixels, height_pixels) %>% unlist %>% as.vector
    nl<-c( seq(0,shape[1]-SIZE, SIZE), shape[1]-SIZE)
    nc<-c( seq(0,shape[2]-SIZE, SIZE) , shape[2]-SIZE)
    nn<-expand.grid(nl,nc)
      max_index <- nrow(nn) 

generator <- function(im,nn, min_index, max_index,batch_size =4, size = SIZE) {
  i <- min_index 
  
    makePng<-function(a,b){
      dest<- paste0(SIZE,"x",SIZE, "+",a, "+", b)
      xim<-  image_crop(im,dest)
      xmask<-  image_crop(mask,dest)
      return(list(xim,xmask))
    }
    
  function() {
     if (i + batch_size >= max_index)
        i <<- min_index 
      rows <- c(i:min(i+batch_size-1, max_index))
      i <<- i + length(rows)
  

    out<-map2(nn[rows,1],nn[rows,2],makePng)
    list(out$xim, out$xmask)
  }
}
```


See ImageMagick Manual for details about the syntax specification. Examples of geometry strings:

"500x300" – Resize image keeping aspect ratio, such that width does not exceed 500 and the height does not exceed 300.

"500x300!" – Resize image to 500 by 300, ignoring aspect ratio

"500x" – Resize width to 500 keep aspect ratio

"x300" – Resize height to 300 keep aspect ratio

"50%x20%" – Resize width to 50 percent and height to 20 percent of original

"500x300+10+20" – Crop image to 500 by 300 at position 10,20
image_write(tiger, path = "tiger.png", format = "png")
image_crop(im, "100x150+50"): crop out width:100px and height:150px starting +50px from the left

```{r}
https://blogs.rstudio.com/ai/posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks/
 https://jjallaire.github.io/deep-learning-with-r-notebooks/notebooks/6.3-advanced-usage-of-recurrent-neural-networks.nb.html 
  
generator <- function(data, lookback, delay, min_index, max_index,
                      shuffle = FALSE, batch_size = 128, step = 6) {
  if (is.null(max_index))
    max_index <- nrow(data) - delay - 1
  i <- min_index + lookback
  function() {
    if (shuffle) {
      rows <- sample(c((min_index+lookback):max_index), size = batch_size)
    } else {
      if (i + batch_size >= max_index)
        i <<- min_index + lookback
      rows <- c(i:min(i+batch_size-1, max_index))
      i <<- i + length(rows)
    }

    samples <- array(0, dim = c(length(rows),
                                lookback / step,
                                dim(data)[[-1]]))
    targets <- array(0, dim = c(length(rows)))
                      
    for (j in 1:length(rows)) {
      indices <- seq(rows[[j]] - lookback, rows[[j]]-1,
                     length.out = dim(samples)[[2]])
      samples[j,,] <- data[indices,]
      targets[[j]] <- data[rows[[j]] + delay,2]
    }           
    list(samples, targets)
  }
}

lookback <- 1440
step <- 6
delay <- 144
batch_size <- 128

train_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = 200000,
  shuffle = TRUE,
  step = step, 
  batch_size = batch_size
)

val_gen = generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 200001,
  max_index = 300000,
  step = step,
  batch_size = batch_size
)

test_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 300001,
  max_index = NULL,
  step = step,
  batch_size = batch_size
)

# How many steps to draw from val_gen in order to see the entire validation set
val_steps <- (300000 - 200001 - lookback) / batch_size

# How many steps to draw from test_gen in order to see the entire test set
test_steps <- (nrow(data) - 300001 - lookback) / batch_size


```


```{r}
model <- keras_model_sequential() %>% 
  layer_flatten(input_shape = c(lookback / step, dim(data)[-1])) %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 500,
  epochs = 20,
  validation_data = val_gen,
  validation_steps = val_steps
)
```




```{r}
k=6

dj$anato[[k]]$properties
ccs<- dj$anato[[k]]$geometry$coordinates [[1]] %>% .[1,,] %>% as.data.frame()
names(ccs) <- c( "x","y")
ccs$value = 1
ftiff<- list.files(BASET, pattern = "*.tiff")
shape<-dataset %>% subset(image_file == ftiff[k] ) %>% dplyr::select(width_pixels, height_pixels) %>% unlist %>% as.vector
anato<- ccs %>% as.cimg( dims=c(shape,1,1) )
imager::save.image(anato,file.path(BASET,  paste0(baseid[k],"_",
       dj$anato[[k]]$properties$classification$name[1],".png") ) )

impath<-file.path(BASET,paste0(baseid[k],".tiff") )
im<-image_read(impath) %>% .[[1]] %>% as.numeric 
#im<- im  %>% .[[1]] %>% as.numeric %>% aperm(perm = c(2,1,3))
im <- im %>% as.cimg()

imager::save.image(x,file.path(BASET,"test.png"))


im<-image_read(impath) %>% image_convert(format = "png")
res<- point.in.polygon(x$x[1:nl],x$y[1:nl],ccs$V1,ccs$V2)
```































